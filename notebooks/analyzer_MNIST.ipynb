{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d7cb209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, copy, os, shutil, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from resnet import resnet20, resnet32, resnet44\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for loading datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST, FashionMNIST\n",
    "\n",
    "# make a directory for logs\n",
    "if \"logs\" not in os.listdir():\n",
    "    os.mkdir(\"logs\")\n",
    "    \n",
    "# NO FANCY TRICKS -- JUST RESIZE TO 32 x 32!\n",
    "train_transforms = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "    \n",
    "# let's use a batch size of 60000\n",
    "batch_size = 512\n",
    "\n",
    "# USE A GPU IF POSSIBLE!\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a5dd6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Epoch 100 of 100 on Model 450 of 450 in 24.049 seconds.\n"
     ]
    }
   ],
   "source": [
    "# for MNIST, let's see which train and test points each model got correct\n",
    "\n",
    "# begin by loading our data\n",
    "data_train = MNIST(root=\"./data\", train=True, download=True, transform=train_transforms)\n",
    "data_test = MNIST(root=\"./data\", train=False, download=True, transform=train_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "data_dim = 1024\n",
    "\n",
    "# create a dataframe with N + 3 columns (model-name + variant + seed)\n",
    "mnist_train_scores = pd.DataFrame(data=None, columns=[\"arch\", \"variant\", \"seed\", \"epoch\"] \\\n",
    "                                  + list(np.arange(60000)))\n",
    "mnist_test_scores = pd.DataFrame(data=None, columns=[\"arch\", \"variant\", \"seed\", \"epoch\"] \\\n",
    "                                 + list(np.arange(10000)))\n",
    "\n",
    "# what models do we have available for this dataset?\n",
    "model_names = [f for f in sorted(os.listdir(\"models/MNIST\")) if \"seed\" in f]\n",
    "\n",
    "# go thru each of our model\n",
    "for model_num, model_name in enumerate(model_names):\n",
    "    \n",
    "    # start time\n",
    "    start = time.time()\n",
    "    \n",
    "    # first figure out what architecture we need to be loading\n",
    "    if \"cnn\" in model_name:\n",
    "        \n",
    "        # how many modules do we have?\n",
    "        variant = int(model_name.split(\"num-modules=\")[1].split(\"_\")[0])\n",
    "        seed = int(model_name.split(\"seed=\")[1])\n",
    "        model_arch = \"cnn\"\n",
    "        \n",
    "        # load the appropriate architecture\n",
    "        model = utils.MNIST_CNN(num_modules=variant)\n",
    "\n",
    "    elif \"mlp\" in model_name:\n",
    "        \n",
    "        # how many hidden layers do we have?\n",
    "        variant = int(model_name.split(\"num-layers=\")[1].split(\"_\")[0])\n",
    "        seed = int(model_name.split(\"seed=\")[1])\n",
    "        model_arch = \"mlp\"\n",
    "        \n",
    "        # load the appropriate architecture\n",
    "        model = utils.MNIST_MLP(num_layers=variant, data_dim=data_dim)\n",
    "        \n",
    "    elif \"resnet\" in model_name:\n",
    "        \n",
    "        # which resnet variant are we loading?\n",
    "        variant = int(model_name.split(\"variant=\")[1].split(\"_\")[0])\n",
    "        seed = int(model_name.split(\"seed=\")[1])\n",
    "        model_arch = \"resnet\"\n",
    "        \n",
    "        # load the appropriate architecture\n",
    "        if variant == 20:\n",
    "            model = resnet20()\n",
    "        elif variant == 32:\n",
    "            model = resnet32()\n",
    "        elif variant == 44:\n",
    "            model = resnet44()\n",
    "            \n",
    "    # create our row header for this row\n",
    "    header = [model_arch, variant, seed]\n",
    "    \n",
    "    # REVISION: ONLY LOOKING AT THE LAST EPOCH:\n",
    "    for epoch in range(99, 100):\n",
    "        \n",
    "        # load in the weights for this epoch\n",
    "        model.load_state_dict(torch.load(f\"models/MNIST/{model_name}/{str(epoch).zfill(3)}.pth\"))\n",
    "        model.to(device); model.eval()\n",
    "        \n",
    "        ###### TRAINING SET METRICS\n",
    "        \n",
    "        # create a list of one-hot encoded accuracies\n",
    "        train_accs = np.array([])\n",
    "        \n",
    "        # compute accuracy on training set\n",
    "        for data in tqdm(trainloader):\n",
    "            \n",
    "            # unpack our x's and y's -- concatenate if necessary\n",
    "            inputs, labels = data\n",
    "            if model_arch == \"resnet\":\n",
    "                inputs = torch.cat([inputs, inputs, inputs], dim=1)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # do not use grad - make our predictions + record our accuracies\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "                train_accs = np.concatenate([train_accs, (predictions == labels).cpu().numpy()])\n",
    "            \n",
    "        # add to our row\n",
    "        mnist_train_scores.loc[len(mnist_train_scores.index)] = header + [epoch] + list(train_accs)\n",
    "        \n",
    "        ###### TESTING SET METRICS\n",
    "        \n",
    "        # create a list of one-hot encoded accuracies\n",
    "        test_accs = np.array([])\n",
    "        \n",
    "        # compute accuracy on the TEST set\n",
    "        for data in tqdm(testloader):\n",
    "            \n",
    "            # unpack our x's and y's -- concatenate if necessary\n",
    "            inputs, labels = data\n",
    "            if model_arch == \"resnet\":\n",
    "                inputs = torch.cat([inputs, inputs, inputs], dim=1)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # do not use grad - make our predictions + record our accuracies\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "                test_accs = np.concatenate([test_accs, (predictions == labels).cpu().numpy()])\n",
    "            \n",
    "        # add to our row\n",
    "        mnist_test_scores.loc[len(mnist_test_scores.index)] = header + [epoch] + list(test_accs)\n",
    "        \n",
    "    # compute end time\n",
    "    end = time.time()\n",
    "        \n",
    "    # status update\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Finished processing Epoch {str(epoch + 1).zfill(3)} of 100 on Model {str(model_num + 1).zfill(3)} of 450 in {np.round(end - start, 3)} seconds.\")\n",
    "\n",
    "# save our logs at the very end\n",
    "mnist_train_scores.to_csv(\"logs/mnist_train_scores.csv\", index=False)\n",
    "mnist_test_scores.to_csv(\"logs/mnist_test_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Afterburner)",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
