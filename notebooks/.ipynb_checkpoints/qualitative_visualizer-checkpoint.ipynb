{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e4885c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, copy, os, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# for SmoothGrad saliency maps (DO NOT USE MAGNITUDE!)\n",
    "from gradients import SmoothGrad\n",
    "\n",
    "# for loading datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST, FashionMNIST\n",
    "\n",
    "# custom utilities + optimized resnets\n",
    "import utils\n",
    "from resnet import resnet20, resnet32, resnet44\n",
    "\n",
    "# no fancy tricks -- let's keep it simple\n",
    "train_transforms = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "batch_size = 256\n",
    "\n",
    "# create a directory for our figures\n",
    "if \"figures\" not in os.listdir():\n",
    "    os.mkdir(\"figures\")\n",
    "    \n",
    "# subdirectory for our qualitative figures\n",
    "if \"qualitative\" not in os.listdir(\"figures\"):\n",
    "    os.mkdir(\"figures/qualitative\")\n",
    "    \n",
    "# use a GPU if possible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# friendly colors\n",
    "colors = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "          '#f781bf', '#a65628', '#984ea3',\n",
    "          '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f2917",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "fe2dd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our training and test data\n",
    "data_train = MNIST(root=\"./data\", train=True, download=True, transform=train_transforms)\n",
    "data_test = MNIST(root=\"./data\", train=False, download=True, transform=train_transforms)\n",
    "\n",
    "# load in our log files too\n",
    "train_scores = pd.read_csv(\"logs/mnist_train_scores.csv\")\n",
    "test_scores = pd.read_csv(\"logs/mnist_test_scores.csv\")\n",
    "\n",
    "# compute the difficulties of each train and test point\n",
    "train_difficulties = train_scores[train_scores.columns[4:]].mean(axis=0).values\n",
    "test_difficulties = test_scores[test_scores.columns[4:]].mean(axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3ec8a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what dataset are we working with?\n",
    "dataset = \"MNIST\"\n",
    "\n",
    "# let's look at hardest, hard, easier, and easiest images.\n",
    "for quantile in [0.25, 0.50, 0.75, 1.00]:\n",
    "\n",
    "    # go thru each of our 10 classes\n",
    "    for class_label in range(10):\n",
    "\n",
    "        # go thru our train + test splits\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            \n",
    "            # pick the image that we will work with for this figure\n",
    "            if split == \"train\":\n",
    "\n",
    "                # query all data points + difficulties that correspond to this class\n",
    "                class_idxs = np.argwhere(data_train.targets.numpy() == class_label).flatten()\n",
    "                class_train_difficulties = train_difficulties[class_idxs]\n",
    "\n",
    "                # what's the accuracy cutoff for this threshold?\n",
    "                critical_val = np.quantile(np.unique(class_train_difficulties), q=quantile)\n",
    "\n",
    "                # which datapoint are we generating a figure\n",
    "                class_viz_idx = class_idxs[np.abs(class_train_difficulties - critical_val).argmin()]\n",
    "                img = data_train[class_viz_idx][0]\n",
    "            \n",
    "            # ... go for the test set\n",
    "            else:\n",
    "                \n",
    "                # query all data points + difficulties that correspond to this class\n",
    "                class_idxs = np.argwhere(data_test.targets.numpy() == class_label).flatten()\n",
    "                class_test_difficulties = test_difficulties[class_idxs]\n",
    "\n",
    "                # what's the accuracy cutoff for this threshold?\n",
    "                critical_val = np.quantile(np.unique(class_test_difficulties), q=quantile)\n",
    "\n",
    "                # which datapoint are we generating a figure\n",
    "                class_viz_idx = class_idxs[np.abs(class_test_difficulties - critical_val).argmin()]\n",
    "                img = data_test[class_viz_idx][0]\n",
    "                \n",
    "            \n",
    "            ######### LOGISTICS\n",
    "            \n",
    "            # create our foldername + make the requisite folder\n",
    "            foldername = f\"{dataset}_class={class_label}_split={split}_diff-q={quantile}\"\n",
    "            if foldername not in os.listdir(\"figures/qualitative\"):\n",
    "                os.mkdir(f\"figures/qualitative/{foldername}\")\n",
    "                \n",
    "            # visualize the image itself\n",
    "            fig, ax = plt.subplots(dpi=200)\n",
    "            ax.imshow(img.numpy()[0])\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"Class {class_label}, % Correct: {np.round(critical_val, 3)}\", fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/img.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            # now move the image to gpu\n",
    "            img = img.reshape(1, *img.shape)\n",
    "            img = img.to(device)\n",
    "            \n",
    "            \n",
    "            ######### FIGURE 1\n",
    "            \n",
    "            # create a 9 x 3 grid of subplots\n",
    "            fig, ax = plt.subplots(3, 9, dpi=200, figsize=(9, 3))\n",
    "\n",
    "            # go thru all 9 possible models\n",
    "            for j, model_desc in enumerate([\"mlp_num-layers=1\", \"mlp_num-layers=2\", \"mlp_num-layers=3\",\n",
    "                                            \"cnn_num-modules=2\", \"cnn_num-modules=3\", \"cnn_num-modules=4\",\n",
    "                                            \"resnet_variant=20\", \"resnet_variant=32\", \"resnet_variant=44\"]):\n",
    "\n",
    "                # load in the base model based on the specific settings + also the weights\n",
    "                if \"mlp\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    model = utils.MNIST_MLP(num_layers=variant, data_dim=1024)\n",
    "                elif \"cnn\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    model = utils.MNIST_CNN(num_modules=variant)\n",
    "                elif \"resnet\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    if variant == 20:\n",
    "                        model = resnet20()\n",
    "                    elif variant == 32:\n",
    "                        model = resnet32()\n",
    "                    elif variant == 44:\n",
    "                        model = resnet44()\n",
    "\n",
    "                # track how many got it correct\n",
    "                symbs = \"\"\n",
    "\n",
    "                # go thru 3 seeds apiece\n",
    "                for i in range(3):\n",
    "\n",
    "                    # load in the correct weights + set to eval mode\n",
    "                    model.load_state_dict(torch.load(f\"models/MNIST/{model_desc}_seed={str(i).zfill(3)}/099.pth\"))\n",
    "                    model.eval(); model.to(device)\n",
    "\n",
    "                    # compute our SmoothGrad saliency map\n",
    "                    smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "\n",
    "                    # special case for resnet!\n",
    "                    if \"resnet\" in model_desc:\n",
    "                        saliency_map = smooth_grad(torch.cat([img, img, img], dim=1), index=None)\n",
    "                        if model(torch.cat([img, img, img], dim=1)).argmax().item() == class_label:\n",
    "                            correct = True; symb = \"✓\"\n",
    "                        else:\n",
    "                            correct = False; symb = \"✗\"\n",
    "                    else:\n",
    "                        saliency_map = smooth_grad(img, index=None)\n",
    "                        if model(img).argmax().item() == class_label:\n",
    "                            correct = True; symb = \"✓\"\n",
    "                        else:\n",
    "                            correct = False; symb = \"✗\"\n",
    "\n",
    "                    # add the symbol to our string\n",
    "                    symbs += symb\n",
    "\n",
    "                    # make saliency map 2d, if not already\n",
    "                    saliency_map_2d = np.sum(saliency_map, axis=0)\n",
    "\n",
    "                    # show our saliency map\n",
    "                    ax[i, j].imshow(saliency_map_2d, cmap=\"viridis\")\n",
    "                    ax[i, j].set_xticks([])\n",
    "                    ax[i, j].set_yticks([])\n",
    "\n",
    "                    # beautify accordingly\n",
    "                    if j == 0:\n",
    "                        ax[i, j].set_ylabel(f\"Seed {i}\", fontsize=8)\n",
    "\n",
    "                # at the end, let's do our incrementing\n",
    "                model_header = model_desc.split(\"_\")[0].capitalize()\n",
    "                ax[0, j].set_title(f\"{model_header.upper()}-{variant} ({symbs})\", fontsize=6)\n",
    "\n",
    "            # beautify at the very end\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/maps.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            \n",
    "            ######### FIGURE 2\n",
    "            \n",
    "            # get 50 saliency maps for extreme MLP, CNN, and resnet - create a roster of colors\n",
    "            color_codes = ([0] * 50) + ([1] * 50) + ([2] * 50) + ([3] * 50) + ([4] * 50) + ([5] * 50)\n",
    "\n",
    "            # create an array to store all of our saliency maps (flattened)\n",
    "            X_saliency = []\n",
    "\n",
    "            # MLP no. 1\n",
    "            model = utils.MNIST_MLP(num_layers=1, data_dim=1024); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/MNIST/mlp_num-layers=1_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # MLP no. 2\n",
    "            model = utils.MNIST_MLP(num_layers=3, data_dim=1024); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/MNIST/mlp_num-layers=3_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # CNN no. 1\n",
    "            model = utils.MNIST_CNN(num_modules=2); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/MNIST/cnn_num-modules=2_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # CNN no. 2\n",
    "            model = utils.MNIST_CNN(num_modules=4); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/MNIST/cnn_num-modules=4_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet20\n",
    "            model = resnet20(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/MNIST/resnet_variant=20_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(torch.cat([img, img, img], dim=1), index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet44\n",
    "            model = resnet44(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/MNIST/resnet_variant=44_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(torch.cat([img, img, img], dim=1), index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # compute our t-SNE\n",
    "            X_saliency = np.array(X_saliency)\n",
    "            tsne = TSNE(n_components=2, random_state=858)\n",
    "            X_trans = tsne.fit_transform(X_saliency)\n",
    "\n",
    "            # create our figure\n",
    "            fig, ax = plt.subplots(dpi=200)\n",
    "            plt.scatter(X_trans[:,0], X_trans[:,1], c=[colors[color_code] for color_code in color_codes])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"MLP-1\", color=colors[0])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"MLP-3\", color=colors[1])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-2\", color=colors[2])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-4\", color=colors[3])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-20\", color=colors[4])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-44\", color=colors[5])\n",
    "            plt.grid()\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.xlabel(\"T-SNE Component 1\")\n",
    "            plt.ylabel(\"T-SNE Component 2\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/tsne.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "            ######### FIGURE 3\n",
    "        \n",
    "            # get the largest MLP, CNN, and ResNet saliencies\n",
    "            mlp_sals, cnn_sals, res_sals = X_saliency[50:100], X_saliency[150:200], X_saliency[250:300]\n",
    "\n",
    "            # compute cosine similarities of MLP only\n",
    "            cossim_mlp = mlp_sals @ mlp_sals.T\n",
    "            cossim_mlp /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_mlp /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_mlp, np.nan)\n",
    "            cossim_mlp = cossim_mlp.flatten()[~np.isnan(cossim_mlp.flatten())]\n",
    "\n",
    "            # compute cosine similarities of CNN only\n",
    "            cossim_cnn = cnn_sals @ cnn_sals.T\n",
    "            cossim_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_cnn, np.nan)\n",
    "            cossim_cnn = cossim_cnn.flatten()[~np.isnan(cossim_cnn.flatten())]\n",
    "\n",
    "            # compute cosine similarities of RESNET only\n",
    "            cossim_res = res_sals @ res_sals.T\n",
    "            cossim_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_res, np.nan)\n",
    "            cossim_res = cossim_res.flatten()[~np.isnan(cossim_res.flatten())]\n",
    "\n",
    "            # compute cosine similarities of MLP + CNNs\n",
    "            cossim_mlp_cnn = mlp_sals @ cnn_sals.T\n",
    "            cossim_mlp_cnn /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_mlp_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # compute cosine similarities of MLP + RESNET\n",
    "            cossim_mlp_res = mlp_sals @ res_sals.T\n",
    "            cossim_mlp_res /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_mlp_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # compute cosine similarities of CNN + RESNET\n",
    "            cossim_cnn_res = cnn_sals @ res_sals.T\n",
    "            cossim_cnn_res /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # create three subplots here\n",
    "            fig, ax = plt.subplots(1, 3, dpi=200, figsize=(20, 5))\n",
    "\n",
    "            # MLP vs. CNN\n",
    "            sns.kdeplot(cossim_mlp.flatten(), ax=ax[0], color=colors[0], label=\"MLP-3\")\n",
    "            sns.kdeplot(cossim_cnn.flatten(), ax=ax[0], color=colors[1], label=\"CNN-4\")\n",
    "            sns.kdeplot(cossim_mlp_cnn.flatten(), ax=ax[0], color=colors[2], label=\"Between\")\n",
    "            ax[0].legend(fontsize=14)\n",
    "            ax[0].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[0].set_ylabel(\"Density\", fontsize=16)\n",
    "            ax[0].set_title(\"MLP-3 and CNN-4\", fontsize=20)\n",
    "            ax[0].grid()\n",
    "\n",
    "            # MLP vs. RESNET\n",
    "            sns.kdeplot(cossim_mlp.flatten(), ax=ax[1], color=colors[0], label=\"MLP-3\")\n",
    "            sns.kdeplot(cossim_res.flatten(), ax=ax[1], color=colors[1], label=\"RESNET-44\")\n",
    "            sns.kdeplot(cossim_mlp_res.flatten(), ax=ax[1], color=colors[2], label=\"Between\")\n",
    "            ax[1].legend(fontsize=14)\n",
    "            ax[1].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[1].set_ylabel(\"\")\n",
    "            ax[1].set_title(\"MLP-3 and RESNET-44\", fontsize=20)\n",
    "            ax[1].grid()\n",
    "\n",
    "            # CNN vs. RESNET\n",
    "            sns.kdeplot(cossim_cnn.flatten(), ax=ax[2], color=colors[0], label=\"CNN-4\")\n",
    "            sns.kdeplot(cossim_res.flatten(), ax=ax[2], color=colors[1], label=\"RESNET-44\")\n",
    "            sns.kdeplot(cossim_cnn_res.flatten(), ax=ax[2], color=colors[2], label=\"Between\")\n",
    "            ax[2].legend(fontsize=14)\n",
    "            ax[2].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[2].set_ylabel(\"\")\n",
    "            ax[2].set_title(\"CNN-4 and RESNET-44\", fontsize=20)\n",
    "            ax[2].grid()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/kde.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c725e",
   "metadata": {},
   "source": [
    "# FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "786c5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our training and test data\n",
    "data_train = FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transforms)\n",
    "data_test = FashionMNIST(root=\"./data\", train=False, download=True, transform=train_transforms)\n",
    "\n",
    "# load in our log files too\n",
    "train_scores = pd.read_csv(\"logs/fashion-mnist_train_scores.csv\")\n",
    "test_scores = pd.read_csv(\"logs/fashion-mnist_test_scores.csv\")\n",
    "\n",
    "# compute the difficulties of each train and test point\n",
    "train_difficulties = train_scores[train_scores.columns[4:]].mean(axis=0).values\n",
    "test_difficulties = test_scores[test_scores.columns[4:]].mean(axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "951fbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what dataset are we working with?\n",
    "dataset = \"FashionMNIST\"\n",
    "\n",
    "# let's look at hardest, hard, easier, and easiest images.\n",
    "for quantile in [0.25, 0.50, 0.75, 1.00]:\n",
    "\n",
    "    # go thru each of our 10 classes\n",
    "    for class_label in range(10):\n",
    "\n",
    "        # go thru our train + test splits\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            \n",
    "            # pick the image that we will work with for this figure\n",
    "            if split == \"train\":\n",
    "\n",
    "                # query all data points + difficulties that correspond to this class\n",
    "                class_idxs = np.argwhere(data_train.targets.numpy() == class_label).flatten()\n",
    "                class_train_difficulties = train_difficulties[class_idxs]\n",
    "\n",
    "                # what's the accuracy cutoff for this threshold?\n",
    "                critical_val = np.quantile(np.unique(class_train_difficulties), q=quantile)\n",
    "\n",
    "                # which datapoint are we generating a figure\n",
    "                class_viz_idx = class_idxs[np.abs(class_train_difficulties - critical_val).argmin()]\n",
    "                img = data_train[class_viz_idx][0]\n",
    "            \n",
    "            # ... go for the test set\n",
    "            else:\n",
    "                \n",
    "                # query all data points + difficulties that correspond to this class\n",
    "                class_idxs = np.argwhere(data_test.targets.numpy() == class_label).flatten()\n",
    "                class_test_difficulties = test_difficulties[class_idxs]\n",
    "\n",
    "                # what's the accuracy cutoff for this threshold?\n",
    "                critical_val = np.quantile(np.unique(class_test_difficulties), q=quantile)\n",
    "\n",
    "                # which datapoint are we generating a figure\n",
    "                class_viz_idx = class_idxs[np.abs(class_test_difficulties - critical_val).argmin()]\n",
    "                img = data_test[class_viz_idx][0]\n",
    "                \n",
    "            \n",
    "            ######### LOGISTICS\n",
    "            \n",
    "            # create our foldername + make the requisite folder\n",
    "            foldername = f\"{dataset}_class={class_label}_split={split}_diff-q={quantile}\"\n",
    "            if foldername not in os.listdir(\"figures/qualitative\"):\n",
    "                os.mkdir(f\"figures/qualitative/{foldername}\")\n",
    "                \n",
    "            # visualize the image itself\n",
    "            fig, ax = plt.subplots(dpi=200)\n",
    "            ax.imshow(img.numpy()[0])\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"Class {class_label}, % Correct: {np.round(critical_val, 3)}\", fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/img.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            # now move the image to gpu\n",
    "            img = img.reshape(1, *img.shape)\n",
    "            img = img.to(device)\n",
    "            \n",
    "            \n",
    "            ######### FIGURE 1\n",
    "            \n",
    "            # create a 9 x 3 grid of subplots\n",
    "            fig, ax = plt.subplots(3, 9, dpi=200, figsize=(9, 3))\n",
    "\n",
    "            # go thru all 9 possible models\n",
    "            for j, model_desc in enumerate([\"mlp_num-layers=1\", \"mlp_num-layers=2\", \"mlp_num-layers=3\",\n",
    "                                            \"cnn_num-modules=2\", \"cnn_num-modules=3\", \"cnn_num-modules=4\",\n",
    "                                            \"resnet_variant=20\", \"resnet_variant=32\", \"resnet_variant=44\"]):\n",
    "\n",
    "                # load in the base model based on the specific settings + also the weights\n",
    "                if \"mlp\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    model = utils.MNIST_MLP(num_layers=variant, data_dim=1024)\n",
    "                elif \"cnn\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    model = utils.MNIST_CNN(num_modules=variant)\n",
    "                elif \"resnet\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    if variant == 20:\n",
    "                        model = resnet20()\n",
    "                    elif variant == 32:\n",
    "                        model = resnet32()\n",
    "                    elif variant == 44:\n",
    "                        model = resnet44()\n",
    "\n",
    "                # track how many got it correct\n",
    "                symbs = \"\"\n",
    "\n",
    "                # go thru 3 seeds apiece\n",
    "                for i in range(3):\n",
    "\n",
    "                    # load in the correct weights + set to eval mode\n",
    "                    model.load_state_dict(torch.load(f\"models/FashionMNIST/{model_desc}_seed={str(i).zfill(3)}/099.pth\"))\n",
    "                    model.eval(); model.to(device)\n",
    "\n",
    "                    # compute our SmoothGrad saliency map\n",
    "                    smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "\n",
    "                    # special case for resnet!\n",
    "                    if \"resnet\" in model_desc:\n",
    "                        saliency_map = smooth_grad(torch.cat([img, img, img], dim=1), index=None)\n",
    "                        if model(torch.cat([img, img, img], dim=1)).argmax().item() == class_label:\n",
    "                            correct = True; symb = \"✓\"\n",
    "                        else:\n",
    "                            correct = False; symb = \"✗\"\n",
    "                    else:\n",
    "                        saliency_map = smooth_grad(img, index=None)\n",
    "                        if model(img).argmax().item() == class_label:\n",
    "                            correct = True; symb = \"✓\"\n",
    "                        else:\n",
    "                            correct = False; symb = \"✗\"\n",
    "\n",
    "                    # add the symbol to our string\n",
    "                    symbs += symb\n",
    "\n",
    "                    # make saliency map 2d, if not already\n",
    "                    saliency_map_2d = np.sum(saliency_map, axis=0)\n",
    "\n",
    "                    # show our saliency map\n",
    "                    ax[i, j].imshow(saliency_map_2d, cmap=\"viridis\")\n",
    "                    ax[i, j].set_xticks([])\n",
    "                    ax[i, j].set_yticks([])\n",
    "\n",
    "                    # beautify accordingly\n",
    "                    if j == 0:\n",
    "                        ax[i, j].set_ylabel(f\"Seed {i}\", fontsize=8)\n",
    "\n",
    "                # at the end, let's do our incrementing\n",
    "                model_header = model_desc.split(\"_\")[0].capitalize()\n",
    "                ax[0, j].set_title(f\"{model_header.upper()}-{variant} ({symbs})\", fontsize=6)\n",
    "\n",
    "            # beautify at the very end\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/maps.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            \n",
    "            ######### FIGURE 2\n",
    "            \n",
    "            # get 50 saliency maps for extreme MLP, CNN, and resnet - create a roster of colors\n",
    "            color_codes = ([0] * 50) + ([1] * 50) + ([2] * 50) + ([3] * 50) + ([4] * 50) + ([5] * 50)\n",
    "\n",
    "            # create an array to store all of our saliency maps (flattened)\n",
    "            X_saliency = []\n",
    "\n",
    "            # MLP no. 1\n",
    "            model = utils.MNIST_MLP(num_layers=1, data_dim=1024); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/FashionMNIST/mlp_num-layers=1_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # MLP no. 2\n",
    "            model = utils.MNIST_MLP(num_layers=3, data_dim=1024); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/FashionMNIST/mlp_num-layers=3_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # CNN no. 1\n",
    "            model = utils.MNIST_CNN(num_modules=2); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/FashionMNIST/cnn_num-modules=2_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # CNN no. 2\n",
    "            model = utils.MNIST_CNN(num_modules=4); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/FashionMNIST/cnn_num-modules=4_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet20\n",
    "            model = resnet20(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/FashionMNIST/resnet_variant=20_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(torch.cat([img, img, img], dim=1), index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet44\n",
    "            model = resnet44(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/FashionMNIST/resnet_variant=44_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(torch.cat([img, img, img], dim=1), index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # compute our t-SNE\n",
    "            X_saliency = np.array(X_saliency)\n",
    "            tsne = TSNE(n_components=2, random_state=858)\n",
    "            X_trans = tsne.fit_transform(X_saliency)\n",
    "\n",
    "            # create our figure\n",
    "            fig, ax = plt.subplots(dpi=200)\n",
    "            plt.scatter(X_trans[:,0], X_trans[:,1], c=[colors[color_code] for color_code in color_codes])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"MLP-1\", color=colors[0])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"MLP-3\", color=colors[1])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-2\", color=colors[2])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-4\", color=colors[3])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-20\", color=colors[4])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-44\", color=colors[5])\n",
    "            plt.grid()\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.xlabel(\"T-SNE Component 1\")\n",
    "            plt.ylabel(\"T-SNE Component 2\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/tsne.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "            ######### FIGURE 3\n",
    "        \n",
    "            # get the largest MLP, CNN, and ResNet saliencies\n",
    "            mlp_sals, cnn_sals, res_sals = X_saliency[50:100], X_saliency[150:200], X_saliency[250:300]\n",
    "\n",
    "            # compute cosine similarities of MLP only\n",
    "            cossim_mlp = mlp_sals @ mlp_sals.T\n",
    "            cossim_mlp /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_mlp /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_mlp, np.nan)\n",
    "            cossim_mlp = cossim_mlp.flatten()[~np.isnan(cossim_mlp.flatten())]\n",
    "\n",
    "            # compute cosine similarities of CNN only\n",
    "            cossim_cnn = cnn_sals @ cnn_sals.T\n",
    "            cossim_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_cnn, np.nan)\n",
    "            cossim_cnn = cossim_cnn.flatten()[~np.isnan(cossim_cnn.flatten())]\n",
    "\n",
    "            # compute cosine similarities of RESNET only\n",
    "            cossim_res = res_sals @ res_sals.T\n",
    "            cossim_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_res, np.nan)\n",
    "            cossim_res = cossim_res.flatten()[~np.isnan(cossim_res.flatten())]\n",
    "\n",
    "            # compute cosine similarities of MLP + CNNs\n",
    "            cossim_mlp_cnn = mlp_sals @ cnn_sals.T\n",
    "            cossim_mlp_cnn /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_mlp_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # compute cosine similarities of MLP + RESNET\n",
    "            cossim_mlp_res = mlp_sals @ res_sals.T\n",
    "            cossim_mlp_res /= np.sqrt((mlp_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_mlp_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # compute cosine similarities of CNN + RESNET\n",
    "            cossim_cnn_res = cnn_sals @ res_sals.T\n",
    "            cossim_cnn_res /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn_res /= np.sqrt((res_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # create three subplots here\n",
    "            fig, ax = plt.subplots(1, 3, dpi=200, figsize=(20, 5))\n",
    "\n",
    "            # MLP vs. CNN\n",
    "            sns.kdeplot(cossim_mlp.flatten(), ax=ax[0], color=colors[0], label=\"MLP-3\")\n",
    "            sns.kdeplot(cossim_cnn.flatten(), ax=ax[0], color=colors[1], label=\"CNN-4\")\n",
    "            sns.kdeplot(cossim_mlp_cnn.flatten(), ax=ax[0], color=colors[2], label=\"Between\")\n",
    "            ax[0].legend(fontsize=14)\n",
    "            ax[0].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[0].set_ylabel(\"Density\", fontsize=16)\n",
    "            ax[0].set_title(\"MLP-3 and CNN-4\", fontsize=20)\n",
    "            ax[0].grid()\n",
    "\n",
    "            # MLP vs. RESNET\n",
    "            sns.kdeplot(cossim_mlp.flatten(), ax=ax[1], color=colors[0], label=\"MLP-3\")\n",
    "            sns.kdeplot(cossim_res.flatten(), ax=ax[1], color=colors[1], label=\"RESNET-44\")\n",
    "            sns.kdeplot(cossim_mlp_res.flatten(), ax=ax[1], color=colors[2], label=\"Between\")\n",
    "            ax[1].legend(fontsize=14)\n",
    "            ax[1].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[1].set_ylabel(\"\")\n",
    "            ax[1].set_title(\"MLP-3 and RESNET-44\", fontsize=20)\n",
    "            ax[1].grid()\n",
    "\n",
    "            # CNN vs. RESNET\n",
    "            sns.kdeplot(cossim_cnn.flatten(), ax=ax[2], color=colors[0], label=\"CNN-4\")\n",
    "            sns.kdeplot(cossim_res.flatten(), ax=ax[2], color=colors[1], label=\"RESNET-44\")\n",
    "            sns.kdeplot(cossim_cnn_res.flatten(), ax=ax[2], color=colors[2], label=\"Between\")\n",
    "            ax[2].legend(fontsize=14)\n",
    "            ax[2].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[2].set_ylabel(\"\")\n",
    "            ax[2].set_title(\"CNN-4 and RESNET-44\", fontsize=20)\n",
    "            ax[2].grid()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/kde.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251038a8",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "8144eee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load our training and test data\n",
    "data_train = CIFAR10(root=\"./data\", train=True, download=True, transform=train_transforms)\n",
    "data_test = CIFAR10(root=\"./data\", train=False, download=True, transform=train_transforms)\n",
    "\n",
    "# load in our log files too\n",
    "train_scores = pd.read_csv(\"logs/cifar10_train_scores.csv\")\n",
    "test_scores = pd.read_csv(\"logs/cifar10_test_scores.csv\")\n",
    "\n",
    "# compute the difficulties of each train and test point\n",
    "train_difficulties = train_scores[train_scores.columns[4:]].mean(axis=0).values\n",
    "test_difficulties = test_scores[test_scores.columns[4:]].mean(axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0b7722b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what dataset are we working with?\n",
    "dataset = \"CIFAR10\"\n",
    "\n",
    "# let's look at hardest, hard, easier, and easiest images.\n",
    "for quantile in [0.25, 0.50, 0.75, 1.00]:\n",
    "\n",
    "    # go thru each of our 10 classes\n",
    "    for class_label in range(10):\n",
    "\n",
    "        # go thru our train + test splits\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            \n",
    "            # pick the image that we will work with for this figure\n",
    "            if split == \"train\":\n",
    "\n",
    "                # query all data points + difficulties that correspond to this class\n",
    "                class_idxs = np.argwhere(np.array(data_train.targets) == class_label).flatten()\n",
    "                class_train_difficulties = train_difficulties[class_idxs]\n",
    "\n",
    "                # what's the accuracy cutoff for this threshold?\n",
    "                critical_val = np.quantile(np.unique(class_train_difficulties), q=quantile)\n",
    "\n",
    "                # which datapoint are we generating a figure\n",
    "                class_viz_idx = class_idxs[np.abs(class_train_difficulties - critical_val).argmin()]\n",
    "                img = data_train[class_viz_idx][0]\n",
    "            \n",
    "            # ... go for the test set\n",
    "            else:\n",
    "                \n",
    "                # query all data points + difficulties that correspond to this class\n",
    "                class_idxs = np.argwhere(np.array(data_test.targets) == class_label).flatten()\n",
    "                class_test_difficulties = test_difficulties[class_idxs]\n",
    "\n",
    "                # what's the accuracy cutoff for this threshold?\n",
    "                critical_val = np.quantile(np.unique(class_test_difficulties), q=quantile)\n",
    "\n",
    "                # which datapoint are we generating a figure\n",
    "                class_viz_idx = class_idxs[np.abs(class_test_difficulties - critical_val).argmin()]\n",
    "                img = data_test[class_viz_idx][0]\n",
    "                \n",
    "            \n",
    "            ######### LOGISTICS\n",
    "            \n",
    "            # create our foldername + make the requisite folder\n",
    "            foldername = f\"{dataset}_class={class_label}_split={split}_diff-q={quantile}\"\n",
    "            if foldername not in os.listdir(\"figures/qualitative\"):\n",
    "                os.mkdir(f\"figures/qualitative/{foldername}\")\n",
    "                \n",
    "            # visualize the image itself\n",
    "            fig, ax = plt.subplots(dpi=200)\n",
    "            ax.imshow(img.numpy()[0])\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"Class {class_label}, % Correct: {np.round(critical_val, 3)}\", fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/img.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            # now move the image to gpu\n",
    "            img = img.reshape(1, *img.shape)\n",
    "            img = img.to(device)\n",
    "            \n",
    "            \n",
    "            ######### FIGURE 1\n",
    "            \n",
    "            # create a 6 x 3 grid of subplots\n",
    "            fig, ax = plt.subplots(3, 6, dpi=200, figsize=(6, 3))\n",
    "\n",
    "            # go thru all 9 possible models\n",
    "            for j, model_desc in enumerate([\"cnn_params=025k\", \"cnn_params=047k\", \"cnn_params=100k\",\n",
    "                                            \"resnet_variant=20\", \"resnet_variant=32\", \"resnet_variant=44\"]):\n",
    "\n",
    "                # load in the base model based on the specific settings + also the weights\n",
    "                if \"cnn\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1].replace(\"k\", \"\"))\n",
    "                    if variant == 25:\n",
    "                        model = utils.CIFAR_CNN25K()\n",
    "                    elif variant == 47:\n",
    "                        model = utils.CIFAR_CNN47K()\n",
    "                    elif variant == 100:\n",
    "                        model = utils.CIFAR_CNN100K()\n",
    "                elif \"resnet\" in model_desc:\n",
    "                    variant = int(model_desc.split(\"=\")[1])\n",
    "                    if variant == 20:\n",
    "                        model = resnet20()\n",
    "                    elif variant == 32:\n",
    "                        model = resnet32()\n",
    "                    elif variant == 44:\n",
    "                        model = resnet44()\n",
    "\n",
    "                # track how many got it correct\n",
    "                symbs = \"\"\n",
    "\n",
    "                # go thru 3 seeds apiece\n",
    "                for i in range(3):\n",
    "\n",
    "                    # load in the correct weights + set to eval mode\n",
    "                    model.load_state_dict(torch.load(f\"models/CIFAR10/{model_desc}_seed={str(i).zfill(3)}/099.pth\"))\n",
    "                    model.eval(); model.to(device)\n",
    "\n",
    "                    # compute our SmoothGrad saliency map\n",
    "                    smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                    saliency_map = smooth_grad(img, index=None)\n",
    "                    if model(img).argmax().item() == class_label:\n",
    "                        correct = True; symb = \"✓\"\n",
    "                    else:\n",
    "                        correct = False; symb = \"✗\"\n",
    "\n",
    "                    # add the symbol to our string\n",
    "                    symbs += symb\n",
    "\n",
    "                    # make saliency map 2d, if not already\n",
    "                    saliency_map_2d = np.sum(saliency_map, axis=0)\n",
    "\n",
    "                    # show our saliency map\n",
    "                    ax[i, j].imshow(saliency_map_2d, cmap=\"viridis\")\n",
    "                    ax[i, j].set_xticks([])\n",
    "                    ax[i, j].set_yticks([])\n",
    "\n",
    "                    # beautify accordingly\n",
    "                    if j == 0:\n",
    "                        ax[i, j].set_ylabel(f\"Seed {i}\", fontsize=8)\n",
    "\n",
    "                # at the end, let's do our incrementing\n",
    "                model_header = model_desc.split(\"_\")[0].capitalize()\n",
    "                ax[0, j].set_title(f\"{model_header.upper()}-{variant} ({symbs})\", fontsize=6)\n",
    "\n",
    "            # beautify at the very end\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/maps.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "\n",
    "            ######### FIGURE 2\n",
    "            \n",
    "            # get 50 saliency maps for all CNNs + ResNets - create a roster of colors\n",
    "            color_codes = ([0] * 50) + ([1] * 50) + ([2] * 50) + ([3] * 50) + ([4] * 50) + ([5] * 50)\n",
    "\n",
    "            # create an array to store all of our saliency maps (flattened)\n",
    "            X_saliency = []\n",
    "\n",
    "            # CNN no. 1\n",
    "            model = utils.CIFAR_CNN25K(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/CIFAR10/cnn_params=025k_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # CNN no. 2\n",
    "            model = utils.CIFAR_CNN47K(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/CIFAR10/cnn_params=047k_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # CNN no. 3\n",
    "            model = utils.CIFAR_CNN100K(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/CIFAR10/cnn_params=100k_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet20\n",
    "            model = resnet20(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/CIFAR10/resnet_variant=20_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet32\n",
    "            model = resnet32(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/CIFAR10/resnet_variant=32_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # resnet44\n",
    "            model = resnet44(); model.to(device); model.eval()\n",
    "            for i in range(50):\n",
    "\n",
    "                # load in the weights of that iteration\n",
    "                model.load_state_dict(torch.load(f\"models/CIFAR10/resnet_variant=44_seed={str(i).zfill(3)}/099.pth\"))\n",
    "\n",
    "                # compute our SmoothGrad saliency map + add it to our list\n",
    "                smooth_grad = SmoothGrad(pretrained_model=model, cuda=True, n_samples=50, magnitude=False)\n",
    "                saliency_map_2d = np.sum(smooth_grad(img, index=None), axis=0)\n",
    "                X_saliency.append(saliency_map_2d.flatten())\n",
    "\n",
    "            # compute our t-SNE\n",
    "            X_saliency = np.array(X_saliency)\n",
    "            tsne = TSNE(n_components=2, random_state=858)\n",
    "            X_trans = tsne.fit_transform(X_saliency)\n",
    "\n",
    "            # create our figure\n",
    "            fig, ax = plt.subplots(dpi=200)\n",
    "            plt.scatter(X_trans[:,0], X_trans[:,1], c=[colors[color_code] for color_code in color_codes])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-25\", color=colors[0])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-47\", color=colors[1])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"CNN-100\", color=colors[2])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-20\", color=colors[3])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-32\", color=colors[4])\n",
    "            plt.scatter([np.nan], [np.nan], label=\"RESNET-44\", color=colors[5])\n",
    "            plt.grid()\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.xlabel(\"T-SNE Component 1\")\n",
    "            plt.ylabel(\"T-SNE Component 2\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/tsne.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "            \n",
    "            ######### FIGURE 3\n",
    "            \n",
    "            # compare mid-tier CNN vs. smallest + largest ResNets.\n",
    "            cnn_sals, res20_sals, res44_sals = X_saliency[50:100], X_saliency[150:200], X_saliency[250:300]\n",
    "\n",
    "            # compute cosine similarities of MLP only\n",
    "            cossim_cnn = cnn_sals @ cnn_sals.T\n",
    "            cossim_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_cnn, np.nan)\n",
    "            cossim_cnn = cossim_cnn.flatten()[~np.isnan(cossim_cnn.flatten())]\n",
    "\n",
    "            # compute cosine similarities of RESNET-20 ONLY\n",
    "            cossim_res20 = res20_sals @ res20_sals.T\n",
    "            cossim_res20 /= np.sqrt((res20_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_res20 /= np.sqrt((res20_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_res20, np.nan)\n",
    "            cossim_res20 = cossim_res20.flatten()[~np.isnan(cossim_res20.flatten())]\n",
    "\n",
    "            # compute cosine similarities of RESNET-44 ONLY\n",
    "            cossim_res44 = res44_sals @ res44_sals.T\n",
    "            cossim_res44 /= np.sqrt((res44_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_res44 /= np.sqrt((res44_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "            np.fill_diagonal(cossim_res44, np.nan)\n",
    "            cossim_res44 = cossim_res44.flatten()[~np.isnan(cossim_res44.flatten())]\n",
    "\n",
    "            # compute cosine similarities of CNN + RES20\n",
    "            cossim_cnn_res20 = cnn_sals @ res20_sals.T\n",
    "            cossim_cnn_res20 /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn_res20 /= np.sqrt((res20_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # compute cosine similarities of CNN + RES44\n",
    "            cossim_cnn_res44 = cnn_sals @ res44_sals.T\n",
    "            cossim_cnn_res44 /= np.sqrt((cnn_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_cnn_res44 /= np.sqrt((res44_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # compute cosine similarities of RES20 + RES44\n",
    "            cossim_res20_res44 = res20_sals @ res44_sals.T\n",
    "            cossim_res20_res44 /= np.sqrt((res20_sals ** 2).sum(axis=1)).reshape(-1, 1)\n",
    "            cossim_res20_res44 /= np.sqrt((res44_sals ** 2).sum(axis=1)).reshape(1, -1)\n",
    "\n",
    "            # create three subplots here\n",
    "            fig, ax = plt.subplots(1, 3, dpi=200, figsize=(20, 5))\n",
    "\n",
    "            # CNN vs. RES20\n",
    "            sns.kdeplot(cossim_cnn.flatten(), ax=ax[0], color=colors[0], label=\"CNN-47\")\n",
    "            sns.kdeplot(cossim_res20.flatten(), ax=ax[0], color=colors[1], label=\"RESNET-20\")\n",
    "            sns.kdeplot(cossim_cnn_res20.flatten(), ax=ax[0], color=colors[2], label=\"Between\")\n",
    "            ax[0].legend(fontsize=14)\n",
    "            ax[0].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[0].set_ylabel(\"Density\", fontsize=16)\n",
    "            ax[0].set_title(\"CNN-47 and RESNET-20\", fontsize=20)\n",
    "            ax[0].grid()\n",
    "\n",
    "            # CNN vs. RES44\n",
    "            sns.kdeplot(cossim_cnn.flatten(), ax=ax[1], color=colors[0], label=\"CNN-47\")\n",
    "            sns.kdeplot(cossim_res44.flatten(), ax=ax[1], color=colors[1], label=\"RESNET-44\")\n",
    "            sns.kdeplot(cossim_cnn_res44.flatten(), ax=ax[1], color=colors[2], label=\"Between\")\n",
    "            ax[1].legend(fontsize=14)\n",
    "            ax[1].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[1].set_ylabel(\"\")\n",
    "            ax[1].set_title(\"CNN-47 and RESNET-44\", fontsize=20)\n",
    "            ax[1].grid()\n",
    "\n",
    "            # RES20 vs. RES44\n",
    "            sns.kdeplot(cossim_res20.flatten(), ax=ax[2], color=colors[0], label=\"RESNET-20\")\n",
    "            sns.kdeplot(cossim_res44.flatten(), ax=ax[2], color=colors[1], label=\"RESNET-44\")\n",
    "            sns.kdeplot(cossim_res20_res44.flatten(), ax=ax[2], color=colors[2], label=\"Between\")\n",
    "            ax[2].legend(fontsize=14)\n",
    "            ax[2].set_xlabel(\"Cosine Similarity\", fontsize=20)\n",
    "            ax[2].set_ylabel(\"\")\n",
    "            ax[2].set_title(\"RESNET-20 and RESNET-44\", fontsize=20)\n",
    "            ax[2].grid()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/qualitative/{foldername}/kde.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "        \n",
    "            \n",
    "\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Afterburner)",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
